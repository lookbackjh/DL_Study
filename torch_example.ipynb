{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Class 선언\n",
    "- Data: Spam베이스라는 57개의 feature을 통해서 spam이메일인지 아닌지 여부를 확인하는 데이터셋, github 참고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코렙용\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1586713/2198860499.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# torch dataloader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "#train_test_split\n",
    "class SpamDataloader(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "\n",
    "class Spam:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_dir = 'src/dataset/spambase' + '/spambase.data'\n",
    "\n",
    "        pass\n",
    "\n",
    "    def create_data(self):\n",
    "        X = pd.read_csv(self.train_dir, sep=',', header=None)\n",
    "        #data.dropna(axis=1, how='all', inplace=True)\n",
    "        X=X.values\n",
    "        Y=X[:,-1]\n",
    "        X=X[:,:-1]\n",
    "        Y = Y.astype(int)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        X_train = torch.tensor(X_train).float()\n",
    "        Y_train = torch.tensor(Y_train).float()\n",
    "        X_test = torch.tensor(X_test).float()\n",
    "        Y_test = torch.tensor(Y_test).float()\n",
    "        Spam_train = SpamDataloader(X_train, Y_train)\n",
    "        Spam_test = SpamDataloader(X_test, Y_test)\n",
    "        return Spam_train, Spam_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network 선언 이때 생성자에서 하이퍼파라미터를 받을 수 있도록 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size x 10\n",
    "        x = self.linear1(x)\n",
    "        #question\n",
    "        # batchsize x 5\n",
    "        x = self.linear2(x)\n",
    "        # x: batch_size x 1\n",
    "        x = self.sigmoid(x)\n",
    "        # x: batch_size x 1\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class 선언: Training과 Testing 둘다 진행가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, dataloader, optimizer, loss_fn,device):\n",
    "        # model, dataloader, optimizer, loss_fn\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        #self.model=self.model.to(device) #  move the model to the device\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for (x, y) in self.dataloader:\n",
    "                x=x.to(self.device)\n",
    "                y=y.to(self.device)\n",
    "                #print(x.device)\n",
    "                y_pred = self.model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
    "\n",
    "    def test(self,test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for (x, y) in test_dataloader:\n",
    "                x=x.to(self.device)\n",
    "                y=y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                print(\"Test Loss: {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 실행  데이터 처리-> 모델 정의 -> 트레이터 클래스 정의 -> 테스트 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 16.89145278930664\n",
      "Epoch: 1, Loss: 4.5887346267700195\n",
      "Epoch: 2, Loss: 0.728247880935669\n",
      "Epoch: 3, Loss: 0.6062618494033813\n",
      "Epoch: 4, Loss: 0.5634623765945435\n",
      "Epoch: 5, Loss: 0.4743380844593048\n",
      "Epoch: 6, Loss: 0.5775176286697388\n",
      "Epoch: 7, Loss: 0.5427300930023193\n",
      "Epoch: 8, Loss: 0.5289063453674316\n",
      "Epoch: 9, Loss: 0.5700421333312988\n",
      "Epoch: 10, Loss: 0.4670093059539795\n",
      "Epoch: 11, Loss: 0.37780749797821045\n",
      "Epoch: 12, Loss: 0.456510066986084\n",
      "Epoch: 13, Loss: 0.43402424454689026\n",
      "Epoch: 14, Loss: 0.5119271278381348\n",
      "Epoch: 15, Loss: 0.44085758924484253\n",
      "Epoch: 16, Loss: 0.4962500333786011\n",
      "Epoch: 17, Loss: 0.3432738780975342\n",
      "Epoch: 18, Loss: 0.41890835762023926\n",
      "Epoch: 19, Loss: 0.36760735511779785\n",
      "Epoch: 20, Loss: 0.2939673066139221\n",
      "Epoch: 21, Loss: 0.3127586841583252\n",
      "Epoch: 22, Loss: 0.3694421350955963\n",
      "Epoch: 23, Loss: 0.29803088307380676\n",
      "Epoch: 24, Loss: 0.4167473316192627\n",
      "Epoch: 25, Loss: 0.30215269327163696\n",
      "Epoch: 26, Loss: 0.2499956637620926\n",
      "Epoch: 27, Loss: 0.31076866388320923\n",
      "Epoch: 28, Loss: 0.21722280979156494\n",
      "Epoch: 29, Loss: 0.3339271545410156\n",
      "Epoch: 30, Loss: 0.24642062187194824\n",
      "Epoch: 31, Loss: 0.2912559509277344\n",
      "Epoch: 32, Loss: 0.2671770751476288\n",
      "Epoch: 33, Loss: 0.2488439381122589\n",
      "Epoch: 34, Loss: 0.25565794110298157\n",
      "Epoch: 35, Loss: 0.28256380558013916\n",
      "Epoch: 36, Loss: 0.4106776714324951\n",
      "Epoch: 37, Loss: 0.3201877474784851\n",
      "Epoch: 38, Loss: 0.3224736452102661\n",
      "Epoch: 39, Loss: 0.3196949362754822\n",
      "Epoch: 40, Loss: 0.2762693166732788\n",
      "Epoch: 41, Loss: 0.2031702995300293\n",
      "Epoch: 42, Loss: 0.3325001001358032\n",
      "Epoch: 43, Loss: 0.2900186777114868\n",
      "Epoch: 44, Loss: 0.3680374026298523\n",
      "Epoch: 45, Loss: 0.2765378952026367\n",
      "Epoch: 46, Loss: 0.21107003092765808\n",
      "Epoch: 47, Loss: 0.31833890080451965\n",
      "Epoch: 48, Loss: 0.2958676218986511\n",
      "Epoch: 49, Loss: 0.23665690422058105\n",
      "Test Loss: 0.27644750475883484\n"
     ]
    }
   ],
   "source": [
    "spam = Spam()\n",
    "Spam_train,Spam_test = spam.create_data() # X: train, Y: train_label, X: test\n",
    "\n",
    "# Create a dataloader\n",
    "train_dataloader = DataLoader(Spam_train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(Spam_test, batch_size=3000, shuffle=True) # batch_size = 3000을 한 이유는 test data 전체를 한번에 testing 하기 위함임. \n",
    "\n",
    "\n",
    "# Create a model\n",
    "model = SimpleNN(input_dim=57, hidden_dim=10,output_dim= 1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Create an optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Create a loss function\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "model=model.to(device)\n",
    "# Create a trainer\n",
    "trainer = Trainer(model, train_dataloader, optimizer, loss_fn,device)\n",
    "# Train the model\n",
    "trainer.train(num_epochs=50)\n",
    "# Test the model\n",
    "trainer.test(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가로 argparse를 통해 hyperparamter튜닝 조금더 편하게 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.3732531666755676\n",
      "Epoch: 1, Loss: 0.3206535577774048\n",
      "Epoch: 2, Loss: 0.49055129289627075\n",
      "Epoch: 3, Loss: 0.2309817522764206\n",
      "Epoch: 4, Loss: 0.4363638162612915\n",
      "Epoch: 5, Loss: 0.4048258364200592\n",
      "Epoch: 6, Loss: 0.4789782166481018\n",
      "Epoch: 7, Loss: 0.21330952644348145\n",
      "Epoch: 8, Loss: 0.41494715213775635\n",
      "Epoch: 9, Loss: 0.12117276340723038\n",
      "Epoch: 10, Loss: 0.20489202439785004\n",
      "Epoch: 11, Loss: 0.21682344377040863\n",
      "Epoch: 12, Loss: 0.23121750354766846\n",
      "Epoch: 13, Loss: 0.2408123016357422\n",
      "Epoch: 14, Loss: 0.09381414204835892\n",
      "Epoch: 15, Loss: 0.19657880067825317\n",
      "Epoch: 16, Loss: 0.46432480216026306\n",
      "Epoch: 17, Loss: 0.26288583874702454\n",
      "Epoch: 18, Loss: 0.11380885541439056\n",
      "Epoch: 19, Loss: 0.27657991647720337\n",
      "Epoch: 20, Loss: 0.23897024989128113\n",
      "Epoch: 21, Loss: 0.4510171115398407\n",
      "Epoch: 22, Loss: 0.2768315374851227\n",
      "Epoch: 23, Loss: 0.19579559564590454\n",
      "Epoch: 24, Loss: 0.18045809864997864\n",
      "Epoch: 25, Loss: 0.24488452076911926\n",
      "Epoch: 26, Loss: 0.24081039428710938\n",
      "Epoch: 27, Loss: 0.15467652678489685\n",
      "Epoch: 28, Loss: 0.28910693526268005\n",
      "Epoch: 29, Loss: 0.1921890377998352\n",
      "Epoch: 30, Loss: 0.19631394743919373\n",
      "Epoch: 31, Loss: 0.17370730638504028\n",
      "Epoch: 32, Loss: 0.14897525310516357\n",
      "Epoch: 33, Loss: 0.1562907099723816\n",
      "Epoch: 34, Loss: 0.18440672755241394\n",
      "Epoch: 35, Loss: 0.15082620084285736\n",
      "Epoch: 36, Loss: 0.2501137852668762\n",
      "Epoch: 37, Loss: 0.34026551246643066\n",
      "Epoch: 38, Loss: 0.24780885875225067\n",
      "Epoch: 39, Loss: 0.38422805070877075\n",
      "Epoch: 40, Loss: 0.0898803099989891\n",
      "Epoch: 41, Loss: 0.21656309068202972\n",
      "Epoch: 42, Loss: 0.3121147155761719\n",
      "Epoch: 43, Loss: 0.10109134018421173\n",
      "Epoch: 44, Loss: 0.15491724014282227\n",
      "Epoch: 45, Loss: 0.3019285202026367\n",
      "Epoch: 46, Loss: 0.15886539220809937\n",
      "Epoch: 47, Loss: 0.1346321851015091\n",
      "Epoch: 48, Loss: 0.14420321583747864\n",
      "Epoch: 49, Loss: 0.12965905666351318\n",
      "Test Loss: 0.21710705757141113\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Spam Classifier')\n",
    "parser.add_argument('--num_hidden', type=int, default=10, help='number of hidden units')\n",
    "parser.add_argument('--num_epochs', type=int, default=50, help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=0.0021, help='learning rate')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "import torch.nn as nn\n",
    "class SimpleNN2(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(57, args.num_hidden)\n",
    "        self.linear2 = nn.Linear(args.num_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: batch_size x 10\n",
    "        x = self.linear1(x)\n",
    "        #question\n",
    "        # batchsize x 5\n",
    "        x = self.linear2(x)\n",
    "        # x: batch_size x 1\n",
    "        x = self.sigmoid(x)\n",
    "        # x: batch_size x 1\n",
    "        return x\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "model = SimpleNN2(args)\n",
    "model=model.to(device)\n",
    "\n",
    "# Create an optimizer\n",
    "class Trainer2():\n",
    "\n",
    "    def __init__(self, model, dataloader, args,device):\n",
    "        # model, dataloader, optimizer, loss_fn\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "        self.device = device\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for (x, y) in self.dataloader:\n",
    "                x=x.to(self.device)\n",
    "                y=y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
    "\n",
    "    def test(self,test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for (x, y) in test_dataloader:\n",
    "                x=x.to(self.device)\n",
    "                y=y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                print(\"Test Loss: {}\".format(loss.item()))\n",
    "    \n",
    "\n",
    "\n",
    "#이런식으로 모델에 args라는 인자를 넣어주어 한꺼번에 사용할 수 있음\n",
    "# Create a model\n",
    "#model = SimpleNN2(args)\n",
    "trainer = Trainer2(model, train_dataloader, args,device)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(num_epochs=50)\n",
    "# Test the model\n",
    "\n",
    "trainer.test(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
