{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시작하기 전에\n",
    "\n",
    "- 제가 준비한것들은 어떻게 보면 파이토치의 원론적인 개념보다는 어떤식으로 파이토치를 활용하고, 어떤 원리를 알아야 파이토치를 사용하기 편한지, 어떤 구조를 가지고 파이토치를 활용할 것인지에 초점을 맞추어서 준비했습니다.\n",
    "\n",
    "- 간단한 컴퓨터 공학적인 요소(클래스 상속)들에 대해서 다루고, 이를 기반으로 어떤식으로 파이토치를 활용하면 좋을지 다뤄봤습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class의 기본적인 속성\n",
    "\n",
    "Pytorch는 Python으로 짜여져 잇으며(밑에단은 C++이지만) python은 객체지향(OOP) 언어입니다 \n",
    "이에 pytorch를 사용하는데 필요할 만한 클래스와 관련한 몇가지 개념을 짚고 넘어가보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 클래스를 사용하는이유\n",
    "- 코드의 재사용성, 즉, 수정 및 변화를 주기에 상당히 유리함\n",
    "- 보기에 간결함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 클래스의 주요 속성: 생성자 및 클래스 메서드\n",
    "- 객체(여러가지 정보를 담고있는 하나의 데이터)를 선언할때, 초기 선언시 필요한 것들을 선언해주면, 해당 정보들을 클래스내 함수 등에서 계속 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m rec1 \u001b[38;5;241m=\u001b[39m Rectangle(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m## width = 3, height = 4를 제공해줌으로써, rec1은 width = 3, height = 4인 직사각형이 된다.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m rec2 \u001b[38;5;241m=\u001b[39m Rectangle(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mrec1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# rec1.print_length()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# rec1.get_area()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#rec2.print_length()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mRectangle.get_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_length\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#length = self.width * 2 + self.height * 2\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#this is same this.width in java    \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#questions: does code below work?\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mwidth\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m height \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "\u001b[0;31mNameError\u001b[0m: name 'width' is not defined"
     ]
    }
   ],
   "source": [
    "class Rectangle():\n",
    "\n",
    "    def __init__(self, width=0, height=0):\n",
    "        # 클래스의 주요 구성요소 1. 속성(attribute)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __str__(self): \n",
    "        if self.width == 0 or self.height == 0:\n",
    "            return \"\"\n",
    "        else:\n",
    "            return ((\"#\" * self.width + \"\\n\") * self.height)[:-1]\n",
    "         # 클래스의 주요 구성요소 2. 메소드(method)\n",
    "    def get_length(self):\n",
    "        length = self.width * 2 + self.height * 2\n",
    "        #this is same this.width in java    \n",
    "\n",
    "        #questions: does code below work?\n",
    "        #length = width * 2 + height * 2\n",
    "        return length\n",
    "\n",
    "    def get_area(self):\n",
    "        area = self.width * self.height\n",
    "        return area\n",
    "    \n",
    "    def print_length(self):\n",
    "        print(\"length: \", self.get_length())\n",
    "        ## questions: does code below work?\n",
    "        # print(\"length: \", get_length())\n",
    "\n",
    "\n",
    "\n",
    "rec1 = Rectangle(3, 4) ## width = 3, height = 4를 제공해줌으로써, rec1은 width = 3, height = 4인 직사각형이 된다.\n",
    "\n",
    "rec2 = Rectangle(6, 7)\n",
    "a=rec1.get_length()\n",
    "print(a)\n",
    "# rec1.print_length()\n",
    "# rec1.get_area()\n",
    "#rec2.print_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 상속(Inheritance)\n",
    "\n",
    "저희가 파이토치를 사용할 경우 저희가 구현하는 클래스들은 `nn.Module` 이라는 클래스를 상속받아서 사용합니다.\n",
    "이를 이해하기 위해서 상속의 필수 개념을 몇가지 되짚어보죠\n",
    "\n",
    "- 중복된 작업의 최소화\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  14\n",
      "area:  12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Shape():\n",
    "\n",
    "    def __init__(self, width=0, height=0):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    def get_length(self):\n",
    "        print(\"3\")\n",
    "        pass\n",
    "    \n",
    "    def get_area(self):\n",
    "        pass\n",
    "\n",
    "    def print_length(self):\n",
    "        print(\"length: \", self.get_length())\n",
    "        print(\"area: \", self.get_area())\n",
    "\n",
    "class Rectangle(Shape): ## this is how inheritance works in python \n",
    "\n",
    "    def __init__(self, width=0, height=0):\n",
    "        super().__init__(width, height)\n",
    "        # this is same as  Shape.__init__(self, width, height)\n",
    "    \n",
    "    def get_length(self):\n",
    "        length = self.width * 2 + self.height * 2\n",
    "        return length\n",
    "    \n",
    "    def get_area(self):\n",
    "        area = self.width * self.height\n",
    "        return area\n",
    "\n",
    "    # def print_length(self):\n",
    "    #     print(\"length: \", self.get_length())\n",
    "    #     print(\"area: \", self.get_area())\n",
    "\n",
    "class Triangle(Shape):\n",
    "\n",
    "    def __init__(self, width=0, height=0):\n",
    "        super().__init__(width, height)\n",
    "    \n",
    "    def get_length(self):\n",
    "        length = self.width * 3\n",
    "        return length\n",
    "    \n",
    "    def get_area(self):\n",
    "        area = self.width * self.height / 2\n",
    "        return area\n",
    "\n",
    "    # def print_length(self):\n",
    "    #     print(\"length: \", self.get_length())\n",
    "    #     print(\"area: \", self.get_area())\n",
    "\n",
    "rec1 = Rectangle(3, 4)\n",
    "#rec2 = Rectangle(6, 7)\n",
    "rec1.print_length()\n",
    "#rec2.print_length()\n",
    "rec1.get_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상속의 특징\n",
    "1. 자식 클래스는 부모 클래스 내부의 메서드를 활용 할 수 있습니다. 위에서 `print_length()` 라는 함수는 부모에서만 정의된 함수인데, 자식인스턴스로 정의하고 활용하였을 때 실행이 되는 것을 확인 할 수 있습니다.\n",
    "2. 부모에서도 정의 되고 자식에서도 정의된 함수가 있으면 자식에서 정의된 함수를 활용합니다 (overriding), 즉 `get_area` 를 호출했을때 자식에서 정의된 `get_area` 를 활용하는 것을 확인 할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토치를 통한 뉴럴 네트워크 구현하기 \n",
    "- 데이터 클래스 만들기\n",
    "- Neural NEt class 만들기\n",
    "- Trainer 클래스 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3071,  0.0501, -0.2627, -0.1737, -0.2734,  0.2100,  0.2581,  0.2083,\n",
      "         -0.1903,  0.0948],\n",
      "        [ 0.0445, -0.1144, -0.2094, -0.1771,  0.2195,  0.2421,  0.1928, -0.1875,\n",
      "          0.2635, -0.0946],\n",
      "        [ 0.2091,  0.1775, -0.1762, -0.0611, -0.1970,  0.1578,  0.2097, -0.1425,\n",
      "         -0.1161,  0.2548],\n",
      "        [-0.2271,  0.0887, -0.1692,  0.1689,  0.2633, -0.1025, -0.2051,  0.0117,\n",
      "          0.1843,  0.1009],\n",
      "        [ 0.0143,  0.0630, -0.2174, -0.1685, -0.0433,  0.2941,  0.1816,  0.1905,\n",
      "          0.0010, -0.1293]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2745,  0.2261,  0.1322, -0.1017,  0.0265], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0584, -0.3708,  0.0841,  0.3775, -0.0717]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0019], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (linear1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (linear2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class SimpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## 보통 일반적으로 사용하는 방법\n",
    "        ## __init__에서 해야할 일 \n",
    "        ## 1. 모델을 구성하는 layer들을 정의\n",
    "        ## 2. Activation 및 Dropout 등을 정의\n",
    "\n",
    "\n",
    "        self.linear1 = nn.Linear(10, 5) # 55 parameters\n",
    "        self.linear2 = nn.Linear(5, 1) # 6 parameters\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.new_params = nn.Parameter(torch.randn(1, 5))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x): # what if I don't have this function?\n",
    "  \n",
    "        ## forward 함수는 모델이 학습데이터를 입력받아서 forward propagation을 진행시키는 함수\n",
    "        ## 모델이 복잡할경우 여러 부분으로 쪼개서 모델을 구성하는것이 좋음. \n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x    \n",
    "    \n",
    "    def print_params(self):\n",
    "        for param in self.parameters():\n",
    "            print(param)\n",
    "\n",
    "model = SimpleNN()\n",
    "#model.print_params()\n",
    "x=torch.randn(1,10) # 1 batch, 10 features (BATCH_SIZE, INPUT_SIZE)\n",
    "a=model(x) # model.forward(x)와 같음   model(x)는 hook이라는 것에 의해서 model.forward(x)로 연결됨 \n",
    "#b=model.forward(x)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5189]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward 작동원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터로더는 토치가 훈련을 하면서 배치단위로(Suffle혹은 Unshuffle)데이터를 받을 때 편하게 할수 있도록 하는 것이다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # these methods are required for custom for propagation\n",
    "    # 데이터에 대한 정보를 가지고 있는 class를 의미한다. \n",
    "    # 아래 세가지 fucntion을 필수적으로 구현해야합니다. \n",
    "    def __init__(self, x, y, z=1):\n",
    "        # x: data size x feature size\n",
    "        # y: data size x 1\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# numpy array to tensor\n",
    "\n",
    "# 1epoch = 10 iterations-> 10 forward propagation + 10 backward propagation 1 iteration-> batch size = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 1 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 2 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 3 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 4 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 5 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 6 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 7 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 8 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n",
      "index: 9 x: torch.Size([10, 10]) y: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "#(10,10)x (10,1)y 10개의 데이터가 순차적으로 호출됨. \n",
    "import numpy as np\n",
    "x = np.random.randn(100, 10) # 100 samples, 10 features\n",
    "y = np.random.randn(100, 1) # 100 samples, 1 label\n",
    "dataset=CustomDataset(torch.Tensor(x), torch.Tensor(y))#numpy array to tensor 변환 필요 \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True) # dataloader 은 iterator이다.\n",
    "for i, (x, y) in enumerate(dataloader):\n",
    "    print(\"index:\", i, \"x:\", x.shape, \"y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, dataloader, optimizer, loss_fn):\n",
    "        # model, dataloader, optimizer, loss_fn\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            for x, y in self.dataloader:\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                self.optimizer.zero_grad() #zero_grad()를 호출하지 않으면, gradient가 누적됨.\n",
    "                loss.backward()# backward propagation을 통해서 gradient를 계산\n",
    "                self.optimizer.step()# step()을 통해서 gradient descent를 수행\n",
    "\n",
    "\n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.44376832246780396\n",
      "Epoch: 1, Loss: 0.5549893379211426\n",
      "Epoch: 2, Loss: 0.953082263469696\n",
      "Epoch: 3, Loss: 1.8867981433868408\n",
      "Epoch: 4, Loss: 1.9228904247283936\n",
      "Epoch: 5, Loss: 2.1852450370788574\n",
      "Epoch: 6, Loss: 0.8915883302688599\n",
      "Epoch: 7, Loss: 1.7382795810699463\n",
      "Epoch: 8, Loss: 1.5107753276824951\n",
      "Epoch: 9, Loss: 1.780603051185608\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(100, 10) # 100 samples, 10 features\n",
    "y = np.random.randn(100, 1) # 100 samples, 1 label\n",
    "\n",
    "dataset=CustomDataset(torch.Tensor(x), torch.Tensor(y))#numpy array to tensor 변환 필요  ## Class 1 : 데이터 관련한 클래스\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "net=SimpleNN() ## Neural Network를 설계해주는 클래스\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "trainer = Trainer(net, dataloader, optimizer, loss_fn) #training을 해주는 클래스 .\n",
    "\n",
    "trainer.train(num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
